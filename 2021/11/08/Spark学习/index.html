<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Spark学习 | Wind</title><meta name="keywords" content="大数据"><meta name="author" content="酷酷的涛"><meta name="copyright" content="酷酷的涛"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Spark概述 官网概述：Spark是一个基于内存的开源计算框架，于2009年诞生于加州大学伯克利分校AMPLab（AMP：Algorithms，Machines，People），它最初属于伯克利大学的研究性项目，后来在2010年正式开源，并于 2013 年成为了 Apache 基金项目，到2014年便成为 Apache 基金的顶级项目，该项目整个发展历程刚过六年时间，但其发展速度非常惊人。正由于">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark学习">
<meta property="og:url" content="http://example.com/2021/11/08/Spark%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="Wind">
<meta property="og:description" content="Spark概述 官网概述：Spark是一个基于内存的开源计算框架，于2009年诞生于加州大学伯克利分校AMPLab（AMP：Algorithms，Machines，People），它最初属于伯克利大学的研究性项目，后来在2010年正式开源，并于 2013 年成为了 Apache 基金项目，到2014年便成为 Apache 基金的顶级项目，该项目整个发展历程刚过六年时间，但其发展速度非常惊人。正由于">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/Fang-gg/picture/img/卡通.jpg">
<meta property="article:published_time" content="2021-11-08T12:52:28.000Z">
<meta property="article:modified_time" content="2021-12-29T02:28:28.267Z">
<meta property="article:author" content="酷酷的涛">
<meta property="article:tag" content="大数据">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/Fang-gg/picture/img/卡通.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2021/11/08/Spark%E5%AD%A6%E4%B9%A0/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: 酷酷的涛","link":"链接: ","source":"来源: Wind","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Spark学习',
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isToc: true,
  postUpdate: '2021-12-29 10:28:28'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if (GLOBAL_CONFIG_SITE.isHome && /iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/gh/metowolf/MetingJS@1.2/dist/Meting.min.js"></script><meta name="generator" content="Hexo 5.4.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://cdn.jsdelivr.net/gh/Fang-gg/picture/img/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20210906160600.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">55</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">7</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/Fang-gg/picture/img/卡通.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Wind</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Spark学习</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-11-08T12:52:28.000Z" title="发表于 2021-11-08 20:52:28">2021-11-08</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-12-29T02:28:28.267Z" title="更新于 2021-12-29 10:28:28">2021-12-29</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0/">大数据学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Spark学习"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><blockquote>
<p>官网概述：<br>Spark是一个基于内存的开源计算框架，于2009年诞生于加州大学伯克利分校AMPLab（AMP：Algorithms，Machines，People），它最初属于伯克利大学的研究性项目，后来在2010年正式开源，并于 2013 年成为了 Apache 基金项目，到2014年便成为 Apache 基金的顶级项目，该项目整个发展历程刚过六年时间，但其发展速度非常惊人。<br>正由于Spark来自于大学，其整个发展过程都充满了学术研究的标记，是学术带动Spark核心架构的发展，如弹性分布式数据集（RDD，resilient distributed datasets）、流处理（Spark streaming）、机器学习（MLlib）、SQL分析（Spark SQL）和图计算（GraphX），本节将主要介绍Spark发展历程和特点。</p>
</blockquote>
<p><img src="https://cdn.jsdelivr.net/gh/Fang-gg/picture/img/image-20211109203608005.png" alt="image-20211109203608005"></p>
<h2 id="SparkWordCount"><a href="#SparkWordCount" class="headerlink" title="SparkWordCount"></a>SparkWordCount</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.fst</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Demo1WordCount</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//Spark配置文件对象</span></span><br><span class="line">    <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置Spark程序的名字</span></span><br><span class="line">    conf.setAppName(<span class="string">&quot;Demo1WordCount&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置运行模式为Local模式  即在IDEA本地运行</span></span><br><span class="line">    conf.setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Spark的上下文环境，相当于是Spark的入口</span></span><br><span class="line">    <span class="keyword">val</span> sc: <span class="type">SparkContext</span> = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 词频统计</span></span><br><span class="line">    <span class="comment">// 1、读取文件</span></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * RDD:弹性分布式数据集</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">val</span> linesRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">&quot;Spark/data/words.txt&quot;</span>)</span><br><span class="line"><span class="comment">//    linesRDD.foreach(println) // 打印出来看看</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2、将每一行的单词切分出来</span></span><br><span class="line">    <span class="comment">// flatMap: 在Spark中称为  算子</span></span><br><span class="line">    <span class="comment">// 算子一般情况下都会返回另外一个RDD</span></span><br><span class="line">    <span class="keyword">val</span> wordsRDD: <span class="type">RDD</span>[<span class="type">String</span>] = linesRDD.flatMap(line =&gt; line.split(<span class="string">&quot;,&quot;</span>))</span><br><span class="line"><span class="comment">//    wordsRDD.foreach(println)  // 打印出来看看</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3、按照单词分组</span></span><br><span class="line">    <span class="keyword">val</span> groupRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Iterable</span>[<span class="type">String</span>])] = wordsRDD.groupBy(word =&gt; word)</span><br><span class="line"><span class="comment">//    groupRDD.foreach(println)  // 打印出来看看</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 4、统计每个单词的数量</span></span><br><span class="line">    <span class="keyword">val</span> countRDD: <span class="type">RDD</span>[<span class="type">String</span>] = groupRDD.map(kv =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> word: <span class="type">String</span> = kv._1</span><br><span class="line">      <span class="keyword">val</span> words: <span class="type">Iterable</span>[<span class="type">String</span>] = kv._2</span><br><span class="line"></span><br><span class="line">      <span class="comment">// words.size 直接获取迭代器的大小</span></span><br><span class="line">      <span class="comment">// 因为相同分组的所有的单词都会到迭代器中</span></span><br><span class="line">      <span class="comment">// 所以迭代器的大小就是单词的数量</span></span><br><span class="line">      word + <span class="string">&quot;,&quot;</span> + words.size</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">      <span class="comment">//5.将结果进行保存</span></span><br><span class="line">    countRDD.saveAsTextFile(<span class="string">&quot;Scala/data/wordCount&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><img src="https://cdn.jsdelivr.net/gh/Fang-gg/picture/img/image-20211108211303061.png" alt="image-20211108211303061"></p>
<p><img src="https://cdn.jsdelivr.net/gh/Fang-gg/picture/img/image-20211108211313806.png" alt="image-20211108211313806"></p>
<p>可以很清楚的看到已经统计出来了。</p>
<h2 id="RDD五大特性"><a href="#RDD五大特性" class="headerlink" title="RDD五大特性"></a>RDD五大特性</h2><ul>
<li>RDD是由一系列分区组成的，默认一个切片对应一个分区</li>
<li>方法(算子)是作用在每一个分区上的，每个分区对应一个Task</li>
<li>RDD之间是有一系列依赖关系的，依赖关系可以分为两种:宽依赖(有shuffle)、窄依赖(没有shuffle)，可以按照宽依赖切分Stage(一组可以并行计算的task，可以看出Map任务或Reduce任务)，shuffle之前是Map，shuffle之后是Reduce</li>
<li>分区类算子只能作用在一个k-v格式的RDD上</li>
<li>Spark给每个task提供最佳的计算位置，移动计算，不移动数据</li>
</ul>
<p>RDD是不存储数据的，它只是一个编程模型，在这之上可以使用各种算子去完成数据分析需求。</p>
<p><font color="red">RDD中分区的数量默认等于上一个RDD分区的数量</font></p>
<p><font color="red">实际上Spark并没有自己读取HDFS文件的方法，底层默认使用的是MR的方式(切片的方式、格式化数据的方式)去将文件读进来</font></p>
<h2 id="Standalone模式搭建"><a href="#Standalone模式搭建" class="headerlink" title="Standalone模式搭建"></a>Standalone模式搭建</h2><p>在公司一般不适用standalone模式，因为公司一般已经有yarn 不需要搞两个资源管理框架，但这里还是搭建一下。</p>
<h3 id="上传，解压并且重命名"><a href="#上传，解压并且重命名" class="headerlink" title="上传，解压并且重命名"></a>上传，解压并且重命名</h3><p><img src="https://cdn.jsdelivr.net/gh/Fang-gg/picture/img/image-20211108221143596.png" alt="image-20211108221143596"></p>
<h3 id="配置环境变量"><a href="#配置环境变量" class="headerlink" title="配置环境变量"></a>配置环境变量</h3><p><img src="https://cdn.jsdelivr.net/gh/Fang-gg/picture/img/image-20211108221422972.png" alt="image-20211108221422972"></p>
<h3 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h3><p><img src="https://cdn.jsdelivr.net/gh/Fang-gg/picture/img/image-20211108221542114.png" alt="image-20211108221542114"></p>
<p><img src="https://cdn.jsdelivr.net/gh/Fang-gg/picture/img/image-20211108221635847.png" alt="image-20211108221635847"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export SPARK_MASTER_IP=master</span><br><span class="line">export SPARK_MASTER_PORT=7077</span><br><span class="line">export SPARK_WORKER_CORES=2</span><br><span class="line">export SPARK_WORKER_INSTANCES=1</span><br><span class="line">export SPARK_WORKER_MEMORY=2g</span><br><span class="line">export JAVA_HOME=/usr/local/soft/jdk1.8.0_171</span><br></pre></td></tr></table></figure>

<h3 id="增加从节点文件"><a href="#增加从节点文件" class="headerlink" title="增加从节点文件"></a>增加从节点文件</h3><p><img src="https://cdn.jsdelivr.net/gh/Fang-gg/picture/img/image-20211108221757819.png" alt="image-20211108221757819"></p>
<p><img src="https://cdn.jsdelivr.net/gh/Fang-gg/picture/img/image-20211108221909721.png" alt="image-20211108221909721"></p>
<h3 id="复制到其他节点"><a href="#复制到其他节点" class="headerlink" title="复制到其他节点"></a>复制到其他节点</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">scp -r spark-2.4.5 node1:`pwd`</span><br><span class="line">scp -r spark-2.4.5 node2:`pwd`</span><br></pre></td></tr></table></figure>

<h3 id="在主节点执行启动命令"><a href="#在主节点执行启动命令" class="headerlink" title="在主节点执行启动命令"></a>在主节点执行启动命令</h3><blockquote>
<p>./sbin/start-all.sh     在spark目录下输入</p>
</blockquote>
<p><img src="https://cdn.jsdelivr.net/gh/Fang-gg/picture/img/image-20211108222723042.png" alt="image-20211108222723042"></p>
<h3 id="访问web界面"><a href="#访问web界面" class="headerlink" title="访问web界面"></a>访问web界面</h3><blockquote>
<p><a target="_blank" rel="noopener" href="http://master:8080/">http://master:8080/</a></p>
</blockquote>
<p><img src="https://cdn.jsdelivr.net/gh/Fang-gg/picture/img/image-20211108222912642.png" alt="image-20211108222912642"></p>
<h3 id="官方示例演示"><a href="#官方示例演示" class="headerlink" title="官方示例演示"></a>官方示例演示</h3><h4 id="standalone-client模式-日志在本地输出，一般用于上线前测试-bin-下执行"><a href="#standalone-client模式-日志在本地输出，一般用于上线前测试-bin-下执行" class="headerlink" title="standalone client模式   日志在本地输出，一般用于上线前测试(bin/下执行)"></a>standalone client模式   日志在本地输出，一般用于上线前测试(bin/下执行)</h4><blockquote>
<p>需要进入到spark-examples_2.11-2.4.5.jar 包所在的目录下执行<br>cd /usr/local/soft/spark-2.4.5/examples/jars</p>
<p>spark-submit –class org.apache.spark.examples.SparkPi –master spark://master:7077 –executor-memory 512m –total-executor-cores 1 spark-examples_2.11-2.4.5.jar 100</p>
</blockquote>
<p><img src="https://cdn.jsdelivr.net/gh/Fang-gg/picture/img/image-20211108223807687.png" alt="image-20211108223807687"></p>
<h4 id="standalone-cluster模式-上线使用，不会再本地打印日志"><a href="#standalone-cluster模式-上线使用，不会再本地打印日志" class="headerlink" title="standalone cluster模式   上线使用，不会再本地打印日志"></a>standalone cluster模式   上线使用，不会再本地打印日志</h4><blockquote>
<p>spark-submit –class org.apache.spark.examples.SparkPi –master spark://master:7077 –driver-memory 512m –deploy-mode cluster –supervise –executor-memory 512M –total-executor-cores 1 spark-examples_2.11-2.4.5.jar 100</p>
</blockquote>
<p><img src="https://cdn.jsdelivr.net/gh/Fang-gg/picture/img/image-20211108223837093.png" alt="image-20211108223837093"></p>
<p>可以很明显的看到两者的区别。</p>
<h4 id="spark-shell-spark-提供的一个交互式的命令行，可以直接写代码"><a href="#spark-shell-spark-提供的一个交互式的命令行，可以直接写代码" class="headerlink" title="spark-shell   spark 提供的一个交互式的命令行，可以直接写代码"></a>spark-shell   spark 提供的一个交互式的命令行，可以直接写代码</h4><blockquote>
<p>spark-shell master spark://master:7077</p>
</blockquote>
<p><img src="https://cdn.jsdelivr.net/gh/Fang-gg/picture/img/image-20211108224415534.png" alt="image-20211108224415534"></p>
<h2 id="整合Yarn"><a href="#整合Yarn" class="headerlink" title="整合Yarn"></a>整合Yarn</h2><blockquote>
<p>停止spark集群<br>在spark sbin目录下执行  ./stop-all.sh</p>
<p>spark整合yarn只需要在一个节点整合, 可以删除node1 和node2中所有的spark 文件</p>
</blockquote>
<h3 id="增加hadoop-配置文件地址"><a href="#增加hadoop-配置文件地址" class="headerlink" title="增加hadoop 配置文件地址"></a>增加hadoop 配置文件地址</h3><blockquote>
<p>vim spark-env.sh<br>增加<br>export HADOOP_CONF_DIR=/usr/local/soft/hadoop-2.7.6/etc/hadoop</p>
</blockquote>
<p><img src="https://cdn.jsdelivr.net/gh/Fang-gg/picture/img/image-20211108225204986.png" alt="image-20211108225204986"></p>
<h3 id="往yarn提交任务需要增加两个配置"><a href="#往yarn提交任务需要增加两个配置" class="headerlink" title="往yarn提交任务需要增加两个配置"></a>往yarn提交任务需要增加两个配置</h3><p>往yarn提交任务需要增加两个配置  yarn-site.xml(/usr/local/soft/hadoop-2.7.6/etc/hadoop/yarn-site.xml)</p>
<p>修改之前先关闭yarn  stop-yarn.sh</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">       &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt;</span><br><span class="line">       &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">   &lt;property&gt;</span><br><span class="line">       &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;</span><br><span class="line">       &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p><img src="https://cdn.jsdelivr.net/gh/Fang-gg/picture/img/image-20211108225553531.png" alt="image-20211108225553531"></p>
<h3 id="同步到其他节点，重启yarn"><a href="#同步到其他节点，重启yarn" class="headerlink" title="同步到其他节点，重启yarn"></a>同步到其他节点，重启yarn</h3><blockquote>
<p>scp -r yarn-site.xml node1:<code>pwd</code><br>scp -r yarn-site.xml node2:<code>pwd</code></p>
</blockquote>
<h3 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h3><blockquote>
<p>cd /usr/local/soft/spark-2.4.5/examples/jars</p>
</blockquote>
<h4 id="spark-on-yarn-client模式-日志在本地输出，一般用于上线前测试"><a href="#spark-on-yarn-client模式-日志在本地输出，一般用于上线前测试" class="headerlink" title="spark on yarn client模式   日志在本地输出，一般用于上线前测试"></a>spark on yarn client模式   日志在本地输出，一般用于上线前测试</h4><blockquote>
<p>spark-submit –class org.apache.spark.examples.SparkPi –master yarn-client –executor-memory 512M –num-executors 2 spark-examples_2.11-2.4.5.jar 100</p>
</blockquote>
<p><img src="https://cdn.jsdelivr.net/gh/Fang-gg/picture/img/image-20211108230440007.png" alt="image-20211108230440007"></p>
<h4 id="spark-on-yarn-cluster模式-上线使用，不会再本地打印日志-减少io"><a href="#spark-on-yarn-cluster模式-上线使用，不会再本地打印日志-减少io" class="headerlink" title="spark on yarn cluster模式   上线使用，不会再本地打印日志   减少io"></a>spark on yarn cluster模式   上线使用，不会再本地打印日志   减少io</h4><blockquote>
<p>spark-submit –class org.apache.spark.examples.SparkPi –master yarn-cluster –executor-memory 512m –num-executors 2 –executor-cores 1 spark-examples_2.11-2.4.5.jar 100</p>
</blockquote>
<h4 id="获取yarn程序执行日志-执行成功之后才能获取到"><a href="#获取yarn程序执行日志-执行成功之后才能获取到" class="headerlink" title="获取yarn程序执行日志  执行成功之后才能获取到"></a>获取yarn程序执行日志  执行成功之后才能获取到</h4><blockquote>
<p>yarn logs -applicationId 你自己的ID</p>
</blockquote>
<p><img src="https://cdn.jsdelivr.net/gh/Fang-gg/picture/img/image-20211108230912123.png" alt="image-20211108230912123"></p>
<p><img src="https://cdn.jsdelivr.net/gh/Fang-gg/picture/img/image-20211108230802439.png" alt="image-20211108230802439"></p>
<p><img src="https://cdn.jsdelivr.net/gh/Fang-gg/picture/img/image-20211108230652667.png" alt="image-20211108230652667"></p>
<h2 id="在集群上运行自己写的Spark代码"><a href="#在集群上运行自己写的Spark代码" class="headerlink" title="在集群上运行自己写的Spark代码"></a>在集群上运行自己写的Spark代码</h2><h3 id="写好代码并且打包成jar包上传到虚拟机"><a href="#写好代码并且打包成jar包上传到虚拟机" class="headerlink" title="写好代码并且打包成jar包上传到虚拟机"></a>写好代码并且打包成jar包上传到虚拟机</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.fst</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.<span class="type">Configuration</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.&#123;<span class="type">FileSystem</span>, <span class="type">Path</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Demo2WordCountSubmit</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 将代码提交到集群上运行</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * 1、去除setMaster(&quot;&quot;local)</span></span><br><span class="line"><span class="comment">     * 2、修改文件的输入输出路径(因为提交到集群默认是从HDFS中获取数据,需要改成HDFS中的路径)</span></span><br><span class="line"><span class="comment">     * 3、在HDFS中创建目录</span></span><br><span class="line"><span class="comment">     * hadoop dfs -mkdir -p /spark/data/words/</span></span><br><span class="line"><span class="comment">     * 5、将程序打成jar包</span></span><br><span class="line"><span class="comment">     *  4、将数据上传至HDFS</span></span><br><span class="line"><span class="comment">     * hadoop dfs -put /usr/local/data/words.txt /spark/data/words/</span></span><br><span class="line"><span class="comment">     * 6、将jar包上传至虚拟机，然后通过spark-submit提交任务</span></span><br><span class="line"><span class="comment">     * spark-submit --class com.fst.Demo2WordCountSubmit --master yarn-client Spark-1.0-SNAPSHOT.jar</span></span><br><span class="line"><span class="comment">     * spark-submit --class com.fst.Demo2WordCountSubmit --master yarn-cluster Spark-1.0-SNAPSHOT.jar</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//Spark配置文件对象</span></span><br><span class="line">    <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置Spark程序的名字</span></span><br><span class="line">    conf.setAppName(<span class="string">&quot;Demo2WordCountSubmit&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置运行模式为Local模式  即在IDEA本地运行</span></span><br><span class="line"><span class="comment">//    conf.setMaster(&quot;local&quot;)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Spark的上下文环境，相当于是Spark的入口</span></span><br><span class="line">    <span class="keyword">val</span> sc: <span class="type">SparkContext</span> = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 词频统计</span></span><br><span class="line">    <span class="comment">// 1、读取文件</span></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * RDD:弹性分布式数据集</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">val</span> linesRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">&quot;/spark/data/words&quot;</span>)</span><br><span class="line"><span class="comment">//    linesRDD.foreach(println) // 打印出来看看</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2、将每一行的单词切分出来</span></span><br><span class="line">    <span class="comment">// flatMap: 在Spark中称为  算子</span></span><br><span class="line">    <span class="comment">// 算子一般情况下都会返回另外一个RDD</span></span><br><span class="line">    <span class="keyword">val</span> wordsRDD: <span class="type">RDD</span>[<span class="type">String</span>] = linesRDD.flatMap(line =&gt; line.split(<span class="string">&quot;,&quot;</span>))</span><br><span class="line"><span class="comment">//    wordsRDD.foreach(println)  // 打印出来看看</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3、按照单词分组</span></span><br><span class="line">    <span class="keyword">val</span> groupRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Iterable</span>[<span class="type">String</span>])] = wordsRDD.groupBy(word =&gt; word)</span><br><span class="line"><span class="comment">//    groupRDD.foreach(println)  // 打印出来看看</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 4、统计每个单词的数量</span></span><br><span class="line">    <span class="keyword">val</span> countRDD: <span class="type">RDD</span>[<span class="type">String</span>] = groupRDD.map(kv =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> word: <span class="type">String</span> = kv._1</span><br><span class="line">      <span class="keyword">val</span> words: <span class="type">Iterable</span>[<span class="type">String</span>] = kv._2</span><br><span class="line"></span><br><span class="line">      <span class="comment">// words.size 直接获取迭代器的大小</span></span><br><span class="line">      <span class="comment">// 因为相同分组的所有的单词都会到迭代器中</span></span><br><span class="line">      <span class="comment">// 所以迭代器的大小就是单词的数量</span></span><br><span class="line">      word + <span class="string">&quot;,&quot;</span> + words.size</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//使用HDFS的JAVA API判断输出路径是否已经存在，存在即删除</span></span><br><span class="line">    <span class="keyword">val</span> hdfsConf: <span class="type">Configuration</span> = <span class="keyword">new</span> <span class="type">Configuration</span>()</span><br><span class="line">    hdfsConf.set(<span class="string">&quot;fs.defaultFs&quot;</span>,<span class="string">&quot;hdfs://master:9000&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> fs: <span class="type">FileSystem</span> = <span class="type">FileSystem</span>.get(hdfsConf)</span><br><span class="line">    <span class="comment">//判断输出路径是否存在</span></span><br><span class="line">    <span class="keyword">if</span>(fs.exists(<span class="keyword">new</span> <span class="type">Path</span>(<span class="string">&quot;/spark/data/wordCount&quot;</span>)))&#123;</span><br><span class="line">      fs.delete(<span class="keyword">new</span> <span class="type">Path</span>(<span class="string">&quot;/spark/data/wordCount&quot;</span>),<span class="literal">true</span>)</span><br><span class="line">    &#125;</span><br><span class="line">      <span class="comment">//5.将结果进行保存</span></span><br><span class="line">    countRDD.saveAsTextFile(<span class="string">&quot;/spark/data/wordCount&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h3><blockquote>
<p>格式: spark-submit –class 主类名 –master yarn的模式(下面展示client模式) 刚上传的jar包</p>
</blockquote>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">spark-submit --<span class="class"><span class="keyword">class</span> <span class="title">com</span>.<span class="title">fst</span>.<span class="title">Demo2WordCountSubmit</span> <span class="title">--master</span> <span class="title">yarn-client</span> <span class="title">Spark-1</span>.0<span class="title">-SNAPSHOT</span>.<span class="title">jar</span></span></span><br></pre></td></tr></table></figure>

<p><img src="https://cdn.jsdelivr.net/gh/Fang-gg/picture/img/image-20211109224300023.png" alt="image-20211109224300023"></p>
<p><img src="https://cdn.jsdelivr.net/gh/Fang-gg/picture/img/image-20211109224311528.png" alt="image-20211109224311528"></p>
<p>可以看到已经是运行完毕了。此时有个小疑问，我们输入的是一个文件，为什么输出会有2个文件勒？这是因为在底层代码逻辑上面，默认有2个并行度，因此会有2个分区，故会有2个文件。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Fang-gg/picture/img/image-20211109230536689.png" alt="image-20211109230536689"></p>
<h2 id="On-Yarn两种模式对比"><a href="#On-Yarn两种模式对比" class="headerlink" title="On Yarn两种模式对比"></a>On Yarn两种模式对比</h2><p>在yarn上有两种模式 <strong>client模式</strong>(会自动打印日志)，<strong>cluster模式</strong>(不会自动打印日志，需要用命令去查看) </p>
<blockquote>
<p>yarn logs -applicationId 你自己的任务ID</p>
</blockquote>
<h3 id="Yarn-client模式"><a href="#Yarn-client模式" class="headerlink" title="Yarn-client模式"></a>Yarn-client模式</h3><p><img src="https://cdn.jsdelivr.net/gh/Fang-gg/picture/img/yarn-client.png" alt="yarn-client"></p>
<p>上图解释:只要提交到集群，那么就会在本地启动JAVA程序。然后就会向ResourceManger发送请求。</p>
<h3 id="Yarn-cluster模式"><a href="#Yarn-cluster模式" class="headerlink" title="Yarn-cluster模式"></a>Yarn-cluster模式</h3><p><img src="https://cdn.jsdelivr.net/gh/Fang-gg/picture/img/yarn-cluster.png" alt="yarn-cluster"></p>
<h2 id="Spark常用算子介绍"><a href="#Spark常用算子介绍" class="headerlink" title="Spark常用算子介绍"></a>Spark常用算子介绍</h2><p><img src="https://cdn.jsdelivr.net/gh/Fang-gg/picture/img/image-20211110111139472.png" alt="image-20211110111139472"></p>
<p>转换算子:将一个RDD变成另一个RDD，RDD之间的转换，懒执行，需要操作算子触发执行</p>
<p>操作算子(行为算子):不能将一个RDD变成另一个RDD，每一个操作算子都会触发一个job</p>
<p>一个Spark程序中可以包含很多个job</p>
<p>可以通过算子的返回值去判断该算子是转换算子还是操作算子</p>
<p>转换算子必须要带上一个操作算子。</p>
<h2 id="相关代码请看我的GitHub个人仓库"><a href="#相关代码请看我的GitHub个人仓库" class="headerlink" title="相关代码请看我的GitHub个人仓库"></a><strong>相关代码请看我的GitHub个人仓库</strong></h2><p><a target="_blank" rel="noopener" href="https://github.com/Fang-gg/Spark.git">https://github.com/Fang-gg/Spark.git</a></p>
<h2 id="Cache缓存"><a href="#Cache缓存" class="headerlink" title="Cache缓存"></a>Cache缓存</h2><p><img src="https://cdn.jsdelivr.net/gh/Fang-gg/picture/img/image-20211111195952229.png" alt="image-20211111195952229"></p>
<ul>
<li>通常情况下，缓存策略的选择<ul>
<li>数据量不大，内存充足的情况下，选择 MEMORY_ONLY</li>
<li>数据量有点大，内存不能完全放下，选择 MEMORY_AND_DISK_SER，尽可能要将数据缓存到内存中，这样的效率是最高的</li>
</ul>
</li>
</ul>
<h2 id="CheckPoint"><a href="#CheckPoint" class="headerlink" title="CheckPoint"></a>CheckPoint</h2><p><img src="https://cdn.jsdelivr.net/gh/Fang-gg/picture/img/checkpoint.png" alt="checkpoint"></p>
<blockquote>
<p>因为checkpoint需要重新启动一个任务进行计算并写入HDFS<br>可以在checkpoint之前 先做一次cache 可以省略计算过程 直接写入HDFS</p>
</blockquote>
<h2 id="累加器和广播变量"><a href="#累加器和广播变量" class="headerlink" title="累加器和广播变量"></a>累加器和广播变量</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.fst</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.util.<span class="type">LongAccumulator</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Demo17Acc</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 累加器</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line">      .setAppName(<span class="string">&quot;Demo17Acc&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sc: <span class="type">SparkContext</span> = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 读取学生数据构建RDD</span></span><br><span class="line">    <span class="keyword">val</span> stuRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">&quot;spark/data/students.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> i: <span class="type">Int</span> = <span class="number">0</span></span><br><span class="line">    println(<span class="string">&quot;i is &quot;</span> + i)</span><br><span class="line">    <span class="comment">// 算子内部的代码 使用了外部变量i</span></span><br><span class="line">    <span class="comment">// 实际上最终封装到task中的是i的一个副本</span></span><br><span class="line">    stuRDD.foreach(line =&gt; &#123;</span><br><span class="line">      i += <span class="number">1</span></span><br><span class="line">      println(line)</span><br><span class="line">    &#125;)</span><br><span class="line">    println(<span class="string">&quot;i is &quot;</span> + i)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果想在算子内部 对外部的变量做一个累加操作</span></span><br><span class="line">    <span class="comment">// 累加器</span></span><br><span class="line">    <span class="comment">// 在算子外面 即Driver端 通过累加器创建一个变量l</span></span><br><span class="line">    <span class="keyword">val</span> l: <span class="type">LongAccumulator</span> = sc.longAccumulator</span><br><span class="line">    stuRDD.foreach(line =&gt; &#123;</span><br><span class="line">      <span class="comment">// 在算子内部使用累加器进行累加</span></span><br><span class="line">      l.add(<span class="number">1</span>)</span><br><span class="line">      println(line)</span><br><span class="line">    &#125;)</span><br><span class="line">    <span class="comment">// 在算子外面 即Driver端 获取累加器最终的结果</span></span><br><span class="line">    println(l.value)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// RDD内部不能再套RDD</span></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 首先RDD是一种抽象的编程模型，并没有实现序列化所以不能进行网络传输</span></span><br><span class="line"><span class="comment">     * 其次就算RDD能够进行网络传输，那如果RDD中还有RDD，</span></span><br><span class="line"><span class="comment">     * 那么需要再task再去申请资源启动Driver、Executor？</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * 如果RDD中套了RDD 就要去整理一下思路，是不是可以转换为其他方式去实现你的逻辑</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="comment">//    stuRDD.foreach(line=&gt;&#123;</span></span><br><span class="line">    <span class="comment">//      stuRDD.foreach(l2=&gt;&#123;</span></span><br><span class="line">    <span class="comment">//        println(l2)</span></span><br><span class="line">    <span class="comment">//      &#125;)</span></span><br><span class="line">    <span class="comment">//    &#125;)</span></span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><img src="https://cdn.jsdelivr.net/gh/Fang-gg/picture/img/%E7%B4%AF%E5%8A%A0%E5%99%A8%E5%92%8C%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F.png" alt="累加器和广播变量"></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.fst</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.broadcast.<span class="type">Broadcast</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Demo18Broadcast</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 广播变量</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line">      .setAppName(<span class="string">&quot;Demo18Broadcast&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sc: <span class="type">SparkContext</span> = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 读取学生数据构建RDD</span></span><br><span class="line">    <span class="keyword">val</span> stuRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">&quot;spark/data/students.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 以学生id构成的List</span></span><br><span class="line">    <span class="keyword">val</span> stuIDs: <span class="type">List</span>[<span class="type">String</span>] = <span class="type">List</span>(<span class="string">&quot;1500100003&quot;</span>, <span class="string">&quot;1500100013&quot;</span>, <span class="string">&quot;1500100023&quot;</span>, <span class="string">&quot;1500100033&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 根据stuIDs在stuRDD中过滤出这些学生的信息</span></span><br><span class="line">    <span class="comment">// 算子内部的代码会被封装成task</span></span><br><span class="line">    <span class="comment">// 相当于每个task中都有一份stuIDs 很明显造成了资源浪费</span></span><br><span class="line">    stuRDD.filter(line =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> id: <span class="type">String</span> = line.split(<span class="string">&quot;,&quot;</span>)(<span class="number">0</span>)</span><br><span class="line">      stuIDs.contains(id)</span><br><span class="line">    &#125;).foreach(println)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 使用广播变量</span></span><br><span class="line">    <span class="comment">// 在Driver端 将stuIDs广播到每一个Executor中</span></span><br><span class="line">    <span class="keyword">val</span> stuIDsBro: <span class="type">Broadcast</span>[<span class="type">List</span>[<span class="type">String</span>]] = sc.broadcast(stuIDs)</span><br><span class="line">    stuRDD.filter(line =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> id: <span class="type">String</span> = line.split(<span class="string">&quot;,&quot;</span>)(<span class="number">0</span>)</span><br><span class="line">      <span class="comment">// 从Executor中获取广播的变量</span></span><br><span class="line">      <span class="keyword">val</span> stuIDsV: <span class="type">List</span>[<span class="type">String</span>] = stuIDsBro.value</span><br><span class="line">      stuIDsV.contains(id)</span><br><span class="line">    &#125;).foreach(println)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="资源调度和任务调度"><a href="#资源调度和任务调度" class="headerlink" title="资源调度和任务调度"></a>资源调度和任务调度</h2><p><img src="https://cdn.jsdelivr.net/gh/Fang-gg/picture/img/%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E5%92%8C%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6.png" alt="资源调度和任务调度"></p>
<h2 id="Shuffle寻址过程"><a href="#Shuffle寻址过程" class="headerlink" title="Shuffle寻址过程"></a>Shuffle寻址过程</h2><p><img src="https://cdn.jsdelivr.net/gh/Fang-gg/picture/img/shuffle%E5%AF%BB%E5%9D%80%E8%BF%87%E7%A8%8B.png" alt="shuffle寻址过程"></p>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/Fang-gg/picture/img/卡通.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/11/19/%E9%A1%B9%E7%9B%AE-%E7%A6%BB%E7%BA%BF%E6%95%B0%E4%BB%93/"><img class="prev-cover" src="https://cdn.jsdelivr.net/gh/Fang-gg/picture/img/异世界.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">项目--离线数仓</div></div></a></div><div class="next-post pull-right"><a href="/2021/11/05/Scala%E5%AD%A6%E4%B9%A0/"><img class="next-cover" src="https://cdn.jsdelivr.net/gh/Fang-gg/picture/img/海贼王.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Scala学习</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2021/10/22/Flume学习小结/" title="Flume学习小结"><img class="cover" src="https://cdn.jsdelivr.net/gh/Fang-gg/picture/img/橘子.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-10-22</div><div class="title">Flume学习小结</div></div></a></div><div><a href="/2021/11/23/Flink/" title="Flink"><img class="cover" src="https://cdn.jsdelivr.net/gh/Fang-gg/picture/img/小孩.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-11-23</div><div class="title">Flink</div></div></a></div><div><a href="/2021/10/09/HBase安装教程/" title="HBase安装教程"><img class="cover" src="https://cdn.jsdelivr.net/gh/Fang-gg/picture/img/童年.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-10-09</div><div class="title">HBase安装教程</div></div></a></div><div><a href="/2021/10/18/HBase的RowKeyy设计/" title="HBase的RowKey设计"><img class="cover" src="https://cdn.jsdelivr.net/gh/Fang-gg/picture/img/窗户.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-10-18</div><div class="title">HBase的RowKey设计</div></div></a></div><div><a href="/2021/09/20/Hadoop学习/" title="Hadoop学习"><img class="cover" src="https://cdn.jsdelivr.net/gh/Fang-gg/picture/img/初音未来.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-09-20</div><div class="title">Hadoop学习</div></div></a></div><div><a href="/2021/10/08/HiveSQL练习题/" title="HiveSQL练习题"><img class="cover" src="https://cdn.jsdelivr.net/gh/Fang-gg/picture/img/列车.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-10-08</div><div class="title">HiveSQL练习题</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Spark"><span class="toc-number">1.</span> <span class="toc-text">Spark</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A6%82%E8%BF%B0"><span class="toc-number">1.1.</span> <span class="toc-text">概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SparkWordCount"><span class="toc-number">1.2.</span> <span class="toc-text">SparkWordCount</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RDD%E4%BA%94%E5%A4%A7%E7%89%B9%E6%80%A7"><span class="toc-number">1.3.</span> <span class="toc-text">RDD五大特性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Standalone%E6%A8%A1%E5%BC%8F%E6%90%AD%E5%BB%BA"><span class="toc-number">1.4.</span> <span class="toc-text">Standalone模式搭建</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8A%E4%BC%A0%EF%BC%8C%E8%A7%A3%E5%8E%8B%E5%B9%B6%E4%B8%94%E9%87%8D%E5%91%BD%E5%90%8D"><span class="toc-number">1.4.1.</span> <span class="toc-text">上传，解压并且重命名</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%85%8D%E7%BD%AE%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F"><span class="toc-number">1.4.2.</span> <span class="toc-text">配置环境变量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BF%AE%E6%94%B9%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="toc-number">1.4.3.</span> <span class="toc-text">修改配置文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A2%9E%E5%8A%A0%E4%BB%8E%E8%8A%82%E7%82%B9%E6%96%87%E4%BB%B6"><span class="toc-number">1.4.4.</span> <span class="toc-text">增加从节点文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%8D%E5%88%B6%E5%88%B0%E5%85%B6%E4%BB%96%E8%8A%82%E7%82%B9"><span class="toc-number">1.4.5.</span> <span class="toc-text">复制到其他节点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9C%A8%E4%B8%BB%E8%8A%82%E7%82%B9%E6%89%A7%E8%A1%8C%E5%90%AF%E5%8A%A8%E5%91%BD%E4%BB%A4"><span class="toc-number">1.4.6.</span> <span class="toc-text">在主节点执行启动命令</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%BF%E9%97%AEweb%E7%95%8C%E9%9D%A2"><span class="toc-number">1.4.7.</span> <span class="toc-text">访问web界面</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%98%E6%96%B9%E7%A4%BA%E4%BE%8B%E6%BC%94%E7%A4%BA"><span class="toc-number">1.4.8.</span> <span class="toc-text">官方示例演示</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#standalone-client%E6%A8%A1%E5%BC%8F-%E6%97%A5%E5%BF%97%E5%9C%A8%E6%9C%AC%E5%9C%B0%E8%BE%93%E5%87%BA%EF%BC%8C%E4%B8%80%E8%88%AC%E7%94%A8%E4%BA%8E%E4%B8%8A%E7%BA%BF%E5%89%8D%E6%B5%8B%E8%AF%95-bin-%E4%B8%8B%E6%89%A7%E8%A1%8C"><span class="toc-number">1.4.8.1.</span> <span class="toc-text">standalone client模式   日志在本地输出，一般用于上线前测试(bin&#x2F;下执行)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#standalone-cluster%E6%A8%A1%E5%BC%8F-%E4%B8%8A%E7%BA%BF%E4%BD%BF%E7%94%A8%EF%BC%8C%E4%B8%8D%E4%BC%9A%E5%86%8D%E6%9C%AC%E5%9C%B0%E6%89%93%E5%8D%B0%E6%97%A5%E5%BF%97"><span class="toc-number">1.4.8.2.</span> <span class="toc-text">standalone cluster模式   上线使用，不会再本地打印日志</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#spark-shell-spark-%E6%8F%90%E4%BE%9B%E7%9A%84%E4%B8%80%E4%B8%AA%E4%BA%A4%E4%BA%92%E5%BC%8F%E7%9A%84%E5%91%BD%E4%BB%A4%E8%A1%8C%EF%BC%8C%E5%8F%AF%E4%BB%A5%E7%9B%B4%E6%8E%A5%E5%86%99%E4%BB%A3%E7%A0%81"><span class="toc-number">1.4.8.3.</span> <span class="toc-text">spark-shell   spark 提供的一个交互式的命令行，可以直接写代码</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B4%E5%90%88Yarn"><span class="toc-number">1.5.</span> <span class="toc-text">整合Yarn</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A2%9E%E5%8A%A0hadoop-%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E5%9C%B0%E5%9D%80"><span class="toc-number">1.5.1.</span> <span class="toc-text">增加hadoop 配置文件地址</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BE%80yarn%E6%8F%90%E4%BA%A4%E4%BB%BB%E5%8A%A1%E9%9C%80%E8%A6%81%E5%A2%9E%E5%8A%A0%E4%B8%A4%E4%B8%AA%E9%85%8D%E7%BD%AE"><span class="toc-number">1.5.2.</span> <span class="toc-text">往yarn提交任务需要增加两个配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%8C%E6%AD%A5%E5%88%B0%E5%85%B6%E4%BB%96%E8%8A%82%E7%82%B9%EF%BC%8C%E9%87%8D%E5%90%AFyarn"><span class="toc-number">1.5.3.</span> <span class="toc-text">同步到其他节点，重启yarn</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A1%88%E4%BE%8B"><span class="toc-number">1.5.4.</span> <span class="toc-text">案例</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#spark-on-yarn-client%E6%A8%A1%E5%BC%8F-%E6%97%A5%E5%BF%97%E5%9C%A8%E6%9C%AC%E5%9C%B0%E8%BE%93%E5%87%BA%EF%BC%8C%E4%B8%80%E8%88%AC%E7%94%A8%E4%BA%8E%E4%B8%8A%E7%BA%BF%E5%89%8D%E6%B5%8B%E8%AF%95"><span class="toc-number">1.5.4.1.</span> <span class="toc-text">spark on yarn client模式   日志在本地输出，一般用于上线前测试</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#spark-on-yarn-cluster%E6%A8%A1%E5%BC%8F-%E4%B8%8A%E7%BA%BF%E4%BD%BF%E7%94%A8%EF%BC%8C%E4%B8%8D%E4%BC%9A%E5%86%8D%E6%9C%AC%E5%9C%B0%E6%89%93%E5%8D%B0%E6%97%A5%E5%BF%97-%E5%87%8F%E5%B0%91io"><span class="toc-number">1.5.4.2.</span> <span class="toc-text">spark on yarn cluster模式   上线使用，不会再本地打印日志   减少io</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%8E%B7%E5%8F%96yarn%E7%A8%8B%E5%BA%8F%E6%89%A7%E8%A1%8C%E6%97%A5%E5%BF%97-%E6%89%A7%E8%A1%8C%E6%88%90%E5%8A%9F%E4%B9%8B%E5%90%8E%E6%89%8D%E8%83%BD%E8%8E%B7%E5%8F%96%E5%88%B0"><span class="toc-number">1.5.4.3.</span> <span class="toc-text">获取yarn程序执行日志  执行成功之后才能获取到</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9C%A8%E9%9B%86%E7%BE%A4%E4%B8%8A%E8%BF%90%E8%A1%8C%E8%87%AA%E5%B7%B1%E5%86%99%E7%9A%84Spark%E4%BB%A3%E7%A0%81"><span class="toc-number">1.6.</span> <span class="toc-text">在集群上运行自己写的Spark代码</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%99%E5%A5%BD%E4%BB%A3%E7%A0%81%E5%B9%B6%E4%B8%94%E6%89%93%E5%8C%85%E6%88%90jar%E5%8C%85%E4%B8%8A%E4%BC%A0%E5%88%B0%E8%99%9A%E6%8B%9F%E6%9C%BA"><span class="toc-number">1.6.1.</span> <span class="toc-text">写好代码并且打包成jar包上传到虚拟机</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%90%E8%A1%8C"><span class="toc-number">1.6.2.</span> <span class="toc-text">运行</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#On-Yarn%E4%B8%A4%E7%A7%8D%E6%A8%A1%E5%BC%8F%E5%AF%B9%E6%AF%94"><span class="toc-number">1.7.</span> <span class="toc-text">On Yarn两种模式对比</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Yarn-client%E6%A8%A1%E5%BC%8F"><span class="toc-number">1.7.1.</span> <span class="toc-text">Yarn-client模式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Yarn-cluster%E6%A8%A1%E5%BC%8F"><span class="toc-number">1.7.2.</span> <span class="toc-text">Yarn-cluster模式</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark%E5%B8%B8%E7%94%A8%E7%AE%97%E5%AD%90%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.8.</span> <span class="toc-text">Spark常用算子介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%B8%E5%85%B3%E4%BB%A3%E7%A0%81%E8%AF%B7%E7%9C%8B%E6%88%91%E7%9A%84GitHub%E4%B8%AA%E4%BA%BA%E4%BB%93%E5%BA%93"><span class="toc-number">1.9.</span> <span class="toc-text">相关代码请看我的GitHub个人仓库</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Cache%E7%BC%93%E5%AD%98"><span class="toc-number">1.10.</span> <span class="toc-text">Cache缓存</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CheckPoint"><span class="toc-number">1.11.</span> <span class="toc-text">CheckPoint</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%B4%AF%E5%8A%A0%E5%99%A8%E5%92%8C%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F"><span class="toc-number">1.12.</span> <span class="toc-text">累加器和广播变量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E5%92%8C%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6"><span class="toc-number">1.13.</span> <span class="toc-text">资源调度和任务调度</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Shuffle%E5%AF%BB%E5%9D%80%E8%BF%87%E7%A8%8B"><span class="toc-number">1.14.</span> <span class="toc-text">Shuffle寻址过程</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image: url('https://cdn.jsdelivr.net/gh/Fang-gg/picture/img/卡通.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2021 - 2022 By 酷酷的涛</div><div class="footer_custom_text">很高兴认识你！</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><script src="/js/search/local-search.js"></script><div class="js-pjax"></div><div class="aplayer no-destroy" data-id="2919023249" data-server="netease" data-type="playlist" data-fixed="true" data-mini="true" data-listFolded="false" data-order="random" data-preload="none" data-autoplay="true" muted></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/gh/metowolf/MetingJS@1.2/dist/Meting.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>